NLP增强的方法有很多种，例如
- 利用同义词词典进行同义词替换
- 随机token位置
- 随机增删token
- 实体替换
- 回译
- 把token【MASK】掉，然后用BERT等预测
- 。。。

其中个人人为最好用的为同义词替换（利用word2vec词表）以及回译
# 同义词替换（利用word2vec词表）
Q: 用word2vec词表进行同义词替换 相对于 直接维护一个同义词表进行同义词替换的优点是什么？  
A:  
能更专注于某个领域，虽然说同义词很简单，但其实现在市面上的同义词表如哈工大的同义词表（链接：https://www.ltp-cloud.com/download），
是一种通用领域的词表。  
当你做具体任务的时候，例如金融领域的文本分类、医学领域的关系抽取时， 通用的同义词表对垂直领域的任务效果并不好。所以一种更"高效"的方式是用word2vec，举个例子，现在要做一个金融领域的文本分类任务。
这时，你就可以爬很多金融类的语料，用gensim、fasttext等库训练一个word2vec词向量，这个词向量表很明显地是"适应"任务的（**这非常重要**）。当我们对一句话进行同义词替换时，例如这句话有10个token，我们随机挑三个，对这三个token寻找word2vec词表余弦距离最近的词替换。

Q：能做得更精细吗？  
A：
在做同义词替换的时候，要避免替换到**关键词**，具体的可以看这个论文《UDA: Unsupervised Data Augmentation for Consistency Training》，怎么限定这个词是关键词呢？可以利用tf-idf的值来进行限定，
如现在设定一个阈值为0.5，当某个词的tf-df值超过这个阀值时，代表这个词是"关键词"，这时候我们就不对它进行替换。

# 回译
回译就很简单了，把原始文本翻译成外语（可以是英文、西班牙语、日语等等），再翻译回中文。这里采用的是百度的通用翻译API。

# 同义词替换和回译的适用场景？  
据我的经验，对于短文本，回译效果比较好。对于长文本，同义词替换效果比较好。